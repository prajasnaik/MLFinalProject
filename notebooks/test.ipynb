{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 15:24:16.691901: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-08 15:24:16.720766: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-08 15:24:16.720794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-08 15:24:16.721738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-08 15:24:16.726213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-08 15:24:17.365524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/label_encoded_telecom_data.csv\")\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "final_columns = [column for column in df.columns if column not in ['customerID', 'tenure', 'TotalCharges', 'MonthlyCharges']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 384)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.encode(final_columns)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_list = []\n",
    "for column_vector in result:\n",
    "    mean_list.append(column_vector.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = ['SeniorCitizen', 'Partner', 'gender', 'Dependents']\n",
    "services = [column for column in df.columns if column not in demographics and column not in ['Contract', 'Churn', 'PaperlessBilling', 'PaymentMethod','customerID', 'tenure', 'TotalCharges', 'MonthlyCharges']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_means = {}\n",
    "services_means = {}\n",
    "for column in df.columns:\n",
    "    if column in demographics:\n",
    "        demographics_means[column] = model.encode(column).mean()\n",
    "    elif column in services:\n",
    "        services_means[column] = model.encode(column).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.drop(services + demographics, axis=1)\n",
    "\n",
    "new_df['service_score'] = df[services].apply(lambda row: sum(row[col] * services_means[col] for col in services), axis=1)\n",
    "new_df['demographics_score'] = df[demographics].apply(lambda row: sum(row[col] * demographics_means[col] for col in demographics), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>tenure</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "      <th>service_score</th>\n",
       "      <th>demographics_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.001074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3962</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2564</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5535</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005915</td>\n",
       "      <td>0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6511</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027</th>\n",
       "      <td>4853</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>84.80</td>\n",
       "      <td>1990.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013577</td>\n",
       "      <td>0.002946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7028</th>\n",
       "      <td>1525</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>103.20</td>\n",
       "      <td>7362.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7029</th>\n",
       "      <td>3367</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.60</td>\n",
       "      <td>346.45</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7030</th>\n",
       "      <td>5934</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>74.40</td>\n",
       "      <td>306.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.003213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7031</th>\n",
       "      <td>2226</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>105.65</td>\n",
       "      <td>6844.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011813</td>\n",
       "      <td>0.001396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7032 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      customerID  tenure  Contract  PaperlessBilling  PaymentMethod  \\\n",
       "0           5375       1         0                 1              2   \n",
       "1           3962      34         1                 0              3   \n",
       "2           2564       2         0                 1              3   \n",
       "3           5535      45         1                 0              0   \n",
       "4           6511       2         0                 1              2   \n",
       "...          ...     ...       ...               ...            ...   \n",
       "7027        4853      24         1                 1              3   \n",
       "7028        1525      72         1                 1              1   \n",
       "7029        3367      11         0                 1              2   \n",
       "7030        5934       4         0                 1              3   \n",
       "7031        2226      66         2                 1              0   \n",
       "\n",
       "      MonthlyCharges  TotalCharges  Churn  service_score  demographics_score  \n",
       "0              29.85         29.85      0       0.003219            0.001074  \n",
       "1              56.95       1889.50      0       0.003268            0.001396  \n",
       "2              53.85        108.15      1       0.004107            0.001396  \n",
       "3              42.30       1840.75      0       0.005915            0.001396  \n",
       "4              70.70        151.65      1       0.000693            0.000000  \n",
       "...              ...           ...    ...            ...                 ...  \n",
       "7027           84.80       1990.50      0       0.013577            0.002946  \n",
       "7028          103.20       7362.90      0       0.012600            0.001550  \n",
       "7029           29.60        346.45      0       0.002842            0.001550  \n",
       "7030           74.40        306.60      1       0.002899            0.003213  \n",
       "7031          105.65       6844.50      0       0.011813            0.001396  \n",
       "\n",
       "[7032 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      gender  SeniorCitizen  Partner  Dependents    tenure  PhoneService  \\\n",
       " 6021       0              0        0           0  0.434674             0   \n",
       " 3404       1              0        0           0 -1.195652             1   \n",
       " 5474       0              0        1           0  0.923772             1   \n",
       " 5515       1              0        1           1  0.516190             1   \n",
       " 6328       0              0        1           1  0.923772             1   \n",
       " \n",
       "       MultipleLines  InternetService  OnlineSecurity  OnlineBackup  \\\n",
       " 6021              1                0               0             0   \n",
       " 3404              0                0               0             0   \n",
       " 5474              2                1               0             0   \n",
       " 5515              0                0               2             0   \n",
       " 6328              2                1               2             0   \n",
       " \n",
       "       DeviceProtection  TechSupport  StreamingTV  StreamingMovies  Contract  \\\n",
       " 6021                 2            0            2                2         0   \n",
       " 3404                 0            0            2                0         0   \n",
       " 5474                 2            0            0                0         0   \n",
       " 5515                 0            2            0                0         2   \n",
       " 6328                 0            0            2                2         1   \n",
       " \n",
       "       PaperlessBilling  PaymentMethod  MonthlyCharges  TotalCharges  \n",
       " 6021                 1              0       -0.518620     -0.087908  \n",
       " 3404                 0              1       -0.374443     -0.921285  \n",
       " 5474                 1              2        0.432616      0.963781  \n",
       " 5515                 0              0       -0.333013      0.122920  \n",
       " 6328                 0              3        1.199902      1.401088  ,\n",
       "       gender  SeniorCitizen  Partner  Dependents    tenure  PhoneService  \\\n",
       " 2476       1              1        1           0  1.168321             1   \n",
       " 6773       0              0        0           0 -0.543522             1   \n",
       " 6116       1              0        1           0 -0.788071             1   \n",
       " 3047       1              0        1           0  0.190125             1   \n",
       " 4092       0              0        0           0 -1.073378             0   \n",
       " \n",
       "       MultipleLines  InternetService  OnlineSecurity  OnlineBackup  \\\n",
       " 2476              2                2               1             1   \n",
       " 6773              2                2               1             1   \n",
       " 6116              2                1               0             0   \n",
       " 3047              2                0               2             0   \n",
       " 4092              1                0               2             0   \n",
       " \n",
       "       DeviceProtection  TechSupport  StreamingTV  StreamingMovies  Contract  \\\n",
       " 2476                 1            1            1                1         2   \n",
       " 6773                 1            1            1                1         0   \n",
       " 6116                 2            0            2                2         0   \n",
       " 3047                 0            0            0                0         0   \n",
       " 4092                 0            0            0                0         0   \n",
       " \n",
       "       PaperlessBilling  PaymentMethod  MonthlyCharges  TotalCharges  \n",
       " 2476                 0              0       -1.315736     -0.341541  \n",
       " 6773                 0              0       -1.325680     -0.798916  \n",
       " 6116                 1              1        1.244647     -0.404569  \n",
       " 3047                 1              3       -0.319756     -0.107975  \n",
       " 4092                 0              0       -1.168245     -0.933316  ,\n",
       " 6021    1\n",
       " 3404    1\n",
       " 5474    1\n",
       " 5515    0\n",
       " 6328    0\n",
       " Name: Churn, dtype: int64,\n",
       " 2476    0\n",
       " 6773    0\n",
       " 6116    1\n",
       " 3047    0\n",
       " 4092    0\n",
       " Name: Churn, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Remove the customerID column\n",
    "new_df = df.drop(columns=['customerID'])\n",
    "\n",
    "# Define the features and target variable\n",
    "X = new_df.drop(columns=['Churn'])\n",
    "y = new_df['Churn']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standard scale the specified columns\n",
    "scaler = StandardScaler()\n",
    "columns_to_scale = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
    "\n",
    "X_train.head(), X_test.head(), y_train.head(), y_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
      "Best F1 Score: 0.7811130915633938\n",
      "Test F1 Score: 0.5848074921956296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.77      4130\n",
      "           1       0.76      0.83      0.79      4130\n",
      "\n",
      "    accuracy                           0.78      8260\n",
      "   macro avg       0.78      0.78      0.78      8260\n",
      "weighted avg       0.78      0.78      0.78      8260\n",
      "\n",
      "[0.10939466 0.62708704 0.         0.00947335 0.16855482 0.04617554\n",
      " 0.0322961  0.00701849]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer, classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the majority and minority classes\n",
    "X_train_majority = X_train[y_train == 0]\n",
    "y_train_majority = y_train[y_train == 0]\n",
    "X_train_minority = X_train[y_train == 1]\n",
    "y_train_minority = y_train[y_train == 1]\n",
    "\n",
    "# Oversample the minority class\n",
    "X_train_minority_oversampled, y_train_minority_oversampled = resample(X_train_minority, y_train_minority, \n",
    "                                                                      replace=True, n_samples=len(X_train_majority), \n",
    "                                                                      random_state=42)\n",
    "\n",
    "# Combine the majority class with the oversampled minority class\n",
    "X_train_oversampled = pd.concat([X_train_majority, X_train_minority_oversampled])\n",
    "y_train_oversampled = pd.concat([y_train_majority, y_train_minority_oversampled])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5,],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a decision tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object with F1 score as the scoring metric\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1, scoring=make_scorer(f1_score))\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best F1 Score:\", best_score)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score on the test set\n",
    "test_f1_score = f1_score(y_test, y_pred)\n",
    "print(\"Test F1 Score:\", test_f1_score)\n",
    "\n",
    "print(classification_report(y_train_oversampled, grid_search.predict(X_train_oversampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances\n",
    "importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "# Get feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances for Decision Tree')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Logistic Regression: {'C': 0.1, 'solver': 'lbfgs'}\n",
      "Best F1 Score for Logistic Regression: 0.7735118541890325\n",
      "Test F1 Score for Logistic Regression: 0.6077457795431976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.70      0.74      4130\n",
      "           1       0.73      0.82      0.77      4130\n",
      "\n",
      "    accuracy                           0.76      8260\n",
      "   macro avg       0.76      0.76      0.76      8260\n",
      "weighted avg       0.76      0.76      0.76      8260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid for logistic regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Create a logistic regression classifier\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Create a GridSearchCV object with F1 score as the scoring metric\n",
    "grid_search_lr = GridSearchCV(estimator=lr, param_grid=param_grid_lr, cv=5, n_jobs=-1, scoring=make_scorer(f1_score))\n",
    "\n",
    "# Fit the model\n",
    "grid_search_lr.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_lr = grid_search_lr.best_params_\n",
    "best_score_lr = grid_search_lr.best_score_\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters for Logistic Regression:\", best_params_lr)\n",
    "print(\"Best F1 Score for Logistic Regression:\", best_score_lr)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lr = grid_search_lr.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score on the test set\n",
    "test_f1_score_lr = f1_score(y_test, y_pred_lr)\n",
    "print(\"Test F1 Score for Logistic Regression:\", test_f1_score_lr)\n",
    "\n",
    "print(classification_report(y_train_oversampled, grid_search_lr.predict(X_train_oversampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.28186433 -1.01427074  0.40451307  0.02373913  0.71521927  0.5392066\n",
      "  -0.10088019  0.01199634]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(grid_search_lr.best_estimator_.coef_)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the coefficients from the best logistic regression model\n",
    "coefficients = grid_search_lr.best_estimator_.coef_[0]\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "coef_df = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort the DataFrame by coefficient value\n",
    "coef_df = coef_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Plot the coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coef_df['Feature'], coef_df['Coefficient'])\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Coefficients for Logistic Regression')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (without service_score and demographics_score): {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best F1 Score (without service_score and demographics_score): 0.7800122382102926\n",
      "Test F1 Score (without service_score and demographics_score): 0.5780933062880325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.77      4130\n",
      "           1       0.75      0.84      0.79      4130\n",
      "\n",
      "    accuracy                           0.78      8260\n",
      "   macro avg       0.79      0.78      0.78      8260\n",
      "weighted avg       0.79      0.78      0.78      8260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the specified columns from X_train and X_test\n",
    "X_train_new = X_train_oversampled.drop(columns=['service_score', 'demographics_score'])\n",
    "X_test_new = X_test.drop(columns=['service_score', 'demographics_score'])\n",
    "\n",
    "# Refit the Decision Tree model\n",
    "grid_search.fit(X_train_new, y_train_oversampled)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_new = grid_search.best_params_\n",
    "best_score_new = grid_search.best_score_\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters (without service_score and demographics_score):\", best_params_new)\n",
    "print(\"Best F1 Score (without service_score and demographics_score):\", best_score_new)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_new = grid_search.predict(X_test_new)\n",
    "\n",
    "# Calculate the F1 score on the test set\n",
    "test_f1_score_new = f1_score(y_test, y_pred_new)\n",
    "print(\"Test F1 Score (without service_score and demographics_score):\", test_f1_score_new)\n",
    "\n",
    "print(classification_report(y_train_oversampled, grid_search.predict(X_train_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6021</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434674</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.518620</td>\n",
       "      <td>-0.087908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.195652</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.374443</td>\n",
       "      <td>-0.921285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5474</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923772</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.432616</td>\n",
       "      <td>0.963781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5515</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.516190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.333013</td>\n",
       "      <td>0.122920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6328</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923772</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.199902</td>\n",
       "      <td>1.401088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  SeniorCitizen  Partner  Dependents  PhoneService  \\\n",
       "6021       0              0        0           0             0   \n",
       "3404       1              0        0           0             1   \n",
       "5474       0              0        1           0             1   \n",
       "5515       1              0        1           1             1   \n",
       "6328       0              0        1           1             1   \n",
       "\n",
       "      PaperlessBilling    tenure  MultipleLines  InternetService  \\\n",
       "6021                 1  0.434674              1                0   \n",
       "3404                 0 -1.195652              0                0   \n",
       "5474                 1  0.923772              2                1   \n",
       "5515                 0  0.516190              0                0   \n",
       "6328                 0  0.923772              2                1   \n",
       "\n",
       "      OnlineSecurity  OnlineBackup  DeviceProtection  TechSupport  \\\n",
       "6021               0             0                 2            0   \n",
       "3404               0             0                 0            0   \n",
       "5474               0             0                 2            0   \n",
       "5515               2             0                 0            2   \n",
       "6328               2             0                 0            0   \n",
       "\n",
       "      StreamingTV  StreamingMovies  Contract  PaymentMethod  MonthlyCharges  \\\n",
       "6021            2                2         0              0       -0.518620   \n",
       "3404            2                0         0              1       -0.374443   \n",
       "5474            0                0         0              2        0.432616   \n",
       "5515            0                0         2              0       -0.333013   \n",
       "6328            2                2         1              3        1.199902   \n",
       "\n",
       "      TotalCharges  \n",
       "6021     -0.087908  \n",
       "3404     -0.921285  \n",
       "5474      0.963781  \n",
       "5515      0.122920  \n",
       "6328      1.401088  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all binary columns\n",
    "binary_columns = [col for col in X_train.columns if X_train[col].nunique() == 2]\n",
    "\n",
    "# Find all non-binary columns\n",
    "non_binary_columns = [col for col in X_train.columns if col not in binary_columns]\n",
    "\n",
    "# Create a new dataframe with binary columns first and then non-binary columns\n",
    "new_X_train = X_train[binary_columns + non_binary_columns]\n",
    "\n",
    "new_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 256\n",
    "train_target = torch.tensor(y_train.values.astype(np.int32))\n",
    "train = torch.tensor(X_train.values.astype(np.float32)) \n",
    "train_tensor = TensorDataset(train, train_target) \n",
    "train_loader = DataLoader(dataset = train_tensor, batch_size = 256, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DeviceDataLoader(train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class GeneratorModel(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        super(GeneratorModel, self).__init__()\n",
    "        self.label_embedding = nn.Embedding(2, 2)\n",
    "        input_dimension = 10 + 2\n",
    "        output_dimension_binary = 6\n",
    "        output_dimension_linear = 13\n",
    "        self.hidden_layer_1 = nn.Sequential(\n",
    "            nn.Linear(input_dimension, 30),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        \n",
    "        self.hidden_layer_2 = nn.Sequential(\n",
    "            nn.Linear(30, 40),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "\n",
    "        self.output_layer1 = nn.Sequential(\n",
    "            nn.Linear(40, output_dimension_binary),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.output_layer2 = nn.Sequential(\n",
    "            nn.Linear(40, output_dimension_linear)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        labels = labels.to(torch.long)\n",
    "        c = self.label_embedding(labels)\n",
    "        x = torch.cat([x,c], 1)\n",
    "        output = self.hidden_layer_1(x)\n",
    "        output = self.hidden_layer_2(output)\n",
    "        output_binary = self.output_layer1(output)\n",
    "        output_linear = self.output_layer2(output)\n",
    "        output = torch.cat([output_binary, output_linear], 1)\n",
    "        return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorModel, self).__init__()\n",
    "        input_dim = 19\n",
    "        output_dim = 1\n",
    "        self.label_embedding = nn.Embedding(2, 2)\n",
    "\n",
    "        self.hidden_layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "\n",
    "        self.hidden_layer2 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "\n",
    "        self.hidden_layer4 = nn.Sequential(\n",
    "            nn.Linear(64, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        output = self.hidden_layer1(x)\n",
    "        output = self.hidden_layer2(output)\n",
    "\n",
    "        output = self.hidden_layer4(output)\n",
    "\n",
    "        return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorModel(\n",
       "  (label_embedding): Embedding(2, 2)\n",
       "  (hidden_layer_1): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=30, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (hidden_layer_2): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=40, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (output_layer1): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=6, bias=True)\n",
       "    (1): Softmax(dim=1)\n",
       "  )\n",
       "  (output_layer2): Sequential(\n",
       "    (0): Linear(in_features=40, out_features=13, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = DiscriminatorModel()\n",
    "generator = GeneratorModel()\n",
    "discriminator.to(device)\n",
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "lossFunction = nn.BCELoss()\n",
    "discriminatorOptim = optim.Adam(discriminator.parameters(), lr = 0.0001)\n",
    "generatorOptim = optim.Adam(generator.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.tensor(np.random.random((real_samples.size(0), 1))).float().to(device)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(torch.FloatTensor(real_samples.shape[0], 1).fill_(1.0).to(device), requires_grad=False)\n",
    "\n",
    "\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data_loader, batch_size, n_epochs, loss, g_opt, d_opt, n_critic = 5):\n",
    "    lambda_gp = 10\n",
    "    mean_g_loss = []\n",
    "    mean_d_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        G_loss = []\n",
    "        D_loss = []\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            batch_size = len(data[1])\n",
    "            noise = torch.randn(batch_size, 10).to(device)\n",
    "            data_class = torch.randint(0, 2, (batch_size, )).float().to(device)\n",
    "\n",
    "\n",
    "            d_opt.zero_grad()\n",
    "\n",
    "\n",
    "            generated_data = generator(noise, data_class)\n",
    "\n",
    "            fake_prediction = discriminator(generated_data).view(batch_size)\n",
    "\n",
    "\n",
    "            true_data_class = Variable(data[1]).to(device)\n",
    "            true_data = data[0].to(device)\n",
    "\n",
    "            real_prediction = discriminator(true_data).view(batch_size)\n",
    "\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, true_data, generated_data)\n",
    "\n",
    "            d_loss = -torch.mean(real_prediction) + torch.mean(fake_prediction) + lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            \n",
    "            d_opt.step()\n",
    "\n",
    "            g_opt.zero_grad()\n",
    "        \n",
    "            if (i % n_critic == 0):\n",
    "\n",
    "                fake = generator(noise, data_class) \n",
    "                fake_prediction = discriminator(fake).view(batch_size)\n",
    "\n",
    "                g_loss = -torch.mean(fake_prediction)\n",
    "\n",
    "                g_loss.backward()\n",
    "                g_opt.step()\n",
    "\n",
    "                print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(data_loader), d_loss.item(), g_loss.item())\n",
    "                )\n",
    "\n",
    "                mean_d_loss.append(d_loss.item())\n",
    "                mean_g_loss.append(g_loss.item())\n",
    "\n",
    "    \n",
    "\n",
    "    plt.plot(mean_g_loss, label= \"Generator Loss\")\n",
    "    plt.plot(mean_d_loss, label= \"Discriminator Loss\")\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Graph of loss')\n",
    "\n",
    "    # Adding a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the graph\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n",
      "[Epoch 0/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 0/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 0/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 0/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 0/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 1/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 1/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 1/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 1/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 1/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 2/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 2/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 2/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 2/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 2/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 3/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 3/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 3/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 3/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 3/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 4/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 4/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 4/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 4/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 4/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 5/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 5/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 5/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 5/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 5/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 6/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 6/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 6/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 6/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 6/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 7/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 7/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 7/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 7/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 7/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 8/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 8/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 8/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 8/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 8/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 9/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 9/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 9/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 9/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 9/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 10/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 10/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 10/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 10/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 10/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 11/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 11/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 11/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 11/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 11/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 12/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 12/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 12/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 12/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 12/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 13/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 13/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 13/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 13/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 13/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 14/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 14/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 14/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 14/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 14/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 15/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 15/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 15/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 15/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 15/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 16/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 16/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 16/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 16/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 16/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 17/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 17/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 17/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 17/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 17/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 18/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 18/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 18/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 18/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 18/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 19/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 19/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 19/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 19/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 19/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 20/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 20/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 20/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 20/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 20/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 21/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 21/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 21/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 21/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 21/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 22/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 22/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 22/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 22/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 22/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 23/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 23/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 23/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 23/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 23/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 24/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 24/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 24/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 24/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 24/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 25/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 25/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 25/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 25/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 25/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 26/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 26/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 26/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 26/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 26/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 27/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 27/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 27/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 27/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 27/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 28/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 28/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 28/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 28/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 28/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 29/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 29/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 29/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 29/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 29/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 30/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 30/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 30/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 30/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 30/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 31/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 31/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 31/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 31/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 31/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 32/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 32/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 32/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 32/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 32/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 33/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 33/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 33/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 33/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 33/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 34/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 34/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 34/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 34/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 34/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 35/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 35/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 35/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 35/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 35/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 36/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 36/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 36/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 36/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 36/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 37/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 37/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 37/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 37/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 37/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 38/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 38/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 38/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 38/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 38/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 39/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 39/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 39/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 39/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 39/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 40/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 40/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 40/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 40/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 40/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 41/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 41/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 41/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 41/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 41/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 42/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 42/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 42/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 42/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 42/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 43/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 43/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 43/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 43/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 43/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 44/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 44/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 44/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 44/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 44/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 45/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 45/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 45/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 45/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 45/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 46/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 46/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 46/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 46/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 46/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 47/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 47/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 47/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 47/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 47/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 48/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 48/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 48/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 48/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 48/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 49/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 49/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 49/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 49/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 49/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 50/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 50/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 50/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 50/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 50/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 51/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 51/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 51/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 51/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 51/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 52/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 52/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 52/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 52/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 52/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 53/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 53/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 53/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 53/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 53/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 54/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 54/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 54/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 54/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 54/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 55/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 55/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 55/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 55/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 55/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 56/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 56/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 56/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 56/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 56/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 57/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 57/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 57/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 57/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 57/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 58/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 58/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 58/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 58/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 58/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 59/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 59/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 59/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 59/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 59/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 60/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 60/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 60/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 60/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 60/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 61/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 61/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 61/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 61/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 61/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 62/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 62/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 62/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 62/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 62/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 63/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 63/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 63/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 63/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 63/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 64/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 64/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 64/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 64/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 64/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 65/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 65/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 65/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 65/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 65/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 66/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 66/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 66/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 66/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 66/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 67/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 67/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 67/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 67/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 67/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 68/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 68/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 68/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 68/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 68/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 69/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 69/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 69/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 69/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 69/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 70/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 70/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 70/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 70/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 70/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 71/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 71/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 71/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 71/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 71/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 72/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 72/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 72/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 72/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 72/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 73/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 73/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 73/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 73/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 73/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 74/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 74/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 74/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 74/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 74/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 75/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 75/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 75/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 75/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 75/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 76/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 76/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 76/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 76/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 76/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 77/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 77/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 77/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 77/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 77/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 78/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 78/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 78/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 78/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 78/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 79/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 79/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 79/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 79/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 79/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 80/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 80/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 80/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 80/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 80/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 81/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 81/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 81/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 81/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 81/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 82/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 82/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 82/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 82/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 82/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 83/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 83/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 83/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 83/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 83/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 84/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 84/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 84/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 84/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 84/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 85/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 85/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 85/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 85/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 85/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 86/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 86/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 86/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 86/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 86/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 87/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 87/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 87/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 87/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 87/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 88/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 88/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 88/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 88/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 88/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 89/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 89/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 89/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 89/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 89/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 90/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 90/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 90/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 90/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 90/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 91/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 91/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 91/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 91/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 91/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 92/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 92/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 92/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 92/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 92/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 93/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 93/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 93/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 93/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 93/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 94/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 94/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 94/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 94/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 94/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 95/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 95/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 95/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 95/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 95/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 96/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 96/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 96/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 96/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 96/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 97/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 97/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 97/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 97/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 97/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 98/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 98/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 98/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 98/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 98/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 99/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 99/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 99/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 99/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 99/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 100/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 100/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 100/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 100/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 100/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 101/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 101/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 101/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 101/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 101/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 102/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 102/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 102/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 102/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 102/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 103/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 103/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 103/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 103/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 103/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 104/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 104/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 104/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 104/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 104/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 105/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 105/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 105/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 105/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 105/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 106/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 106/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 106/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 106/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 106/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 107/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 107/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 107/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 107/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 107/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 108/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 108/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 108/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 108/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 108/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 109/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 109/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 109/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 109/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 109/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 110/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 110/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 110/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 110/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 110/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 111/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 111/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 111/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 111/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 111/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 112/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 112/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 112/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 112/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 112/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 113/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 113/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 113/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 113/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 113/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 114/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 114/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 114/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 114/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 114/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 115/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 115/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 115/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 115/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 115/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 116/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 116/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 116/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 116/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 116/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 117/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 117/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 117/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 117/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 117/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 118/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 118/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 118/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 118/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 118/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 119/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 119/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 119/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 119/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 119/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 120/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 120/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 120/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 120/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 120/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 121/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 121/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 121/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 121/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 121/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 122/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 122/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 122/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 122/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 122/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 123/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 123/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 123/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 123/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 123/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 124/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 124/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 124/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 124/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 124/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 125/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 125/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 125/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 125/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 125/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 126/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 126/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 126/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 126/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 126/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 127/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 127/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 127/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 127/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 127/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 128/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 128/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 128/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 128/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 128/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 129/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 129/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 129/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 129/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 129/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 130/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 130/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 130/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 130/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 130/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 131/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 131/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 131/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 131/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 131/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 132/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 132/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 132/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 132/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 132/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 133/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 133/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 133/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 133/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 133/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 134/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 134/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 134/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 134/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 134/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 135/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 135/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 135/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 135/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 135/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 136/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 136/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 136/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 136/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 136/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 137/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 137/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 137/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 137/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 137/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 138/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 138/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 138/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 138/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 138/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 139/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 139/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 139/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 139/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 139/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 140/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 140/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 140/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 140/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 140/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 141/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 141/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 141/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 141/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 141/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 142/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 142/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 142/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 142/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 142/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 143/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 143/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 143/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 143/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 143/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 144/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 144/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 144/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 144/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 144/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 145/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 145/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 145/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 145/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 145/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 146/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 146/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 146/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 146/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 146/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 147/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 147/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 147/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 147/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 147/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 148/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 148/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 148/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 148/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 148/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 149/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 149/200] [Batch 5/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 149/200] [Batch 10/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 149/200] [Batch 15/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 149/200] [Batch 20/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n",
      "[Epoch 150/200] [Batch 0/22] [D loss: 10.000000] [G loss: -1.000000]\n",
      "torch.float32 torch.float32\n",
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "fit(train_loader, batch_size, 200, lossFunction, generatorOptim, discriminatorOptim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.185042</td>\n",
       "      <td>0.156384</td>\n",
       "      <td>0.178105</td>\n",
       "      <td>0.131436</td>\n",
       "      <td>0.181296</td>\n",
       "      <td>0.167736</td>\n",
       "      <td>-0.016865</td>\n",
       "      <td>-0.105815</td>\n",
       "      <td>-0.053186</td>\n",
       "      <td>-0.172749</td>\n",
       "      <td>-0.059698</td>\n",
       "      <td>-0.030590</td>\n",
       "      <td>-0.077091</td>\n",
       "      <td>-0.028659</td>\n",
       "      <td>-0.241728</td>\n",
       "      <td>-0.076758</td>\n",
       "      <td>0.151002</td>\n",
       "      <td>0.155057</td>\n",
       "      <td>-0.080559</td>\n",
       "      <td>0.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.024163</td>\n",
       "      <td>0.027582</td>\n",
       "      <td>0.018869</td>\n",
       "      <td>0.020795</td>\n",
       "      <td>0.028070</td>\n",
       "      <td>0.151935</td>\n",
       "      <td>0.184663</td>\n",
       "      <td>0.169728</td>\n",
       "      <td>0.191974</td>\n",
       "      <td>0.173430</td>\n",
       "      <td>0.172230</td>\n",
       "      <td>0.171695</td>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.182624</td>\n",
       "      <td>0.142371</td>\n",
       "      <td>0.155359</td>\n",
       "      <td>0.202496</td>\n",
       "      <td>0.165470</td>\n",
       "      <td>0.500006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.084527</td>\n",
       "      <td>0.051687</td>\n",
       "      <td>0.048179</td>\n",
       "      <td>0.054626</td>\n",
       "      <td>0.079699</td>\n",
       "      <td>0.059483</td>\n",
       "      <td>-0.834261</td>\n",
       "      <td>-1.168748</td>\n",
       "      <td>-0.854694</td>\n",
       "      <td>-1.199198</td>\n",
       "      <td>-1.771493</td>\n",
       "      <td>-0.880217</td>\n",
       "      <td>-1.088684</td>\n",
       "      <td>-0.824813</td>\n",
       "      <td>-1.328450</td>\n",
       "      <td>-0.978064</td>\n",
       "      <td>-0.673684</td>\n",
       "      <td>-0.813255</td>\n",
       "      <td>-0.926193</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.169330</td>\n",
       "      <td>0.142467</td>\n",
       "      <td>0.162137</td>\n",
       "      <td>0.120002</td>\n",
       "      <td>0.169915</td>\n",
       "      <td>0.150546</td>\n",
       "      <td>-0.102229</td>\n",
       "      <td>-0.197091</td>\n",
       "      <td>-0.149690</td>\n",
       "      <td>-0.286428</td>\n",
       "      <td>-0.147699</td>\n",
       "      <td>-0.129361</td>\n",
       "      <td>-0.173878</td>\n",
       "      <td>-0.121562</td>\n",
       "      <td>-0.335840</td>\n",
       "      <td>-0.155375</td>\n",
       "      <td>0.062328</td>\n",
       "      <td>0.039139</td>\n",
       "      <td>-0.182759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.183725</td>\n",
       "      <td>0.154332</td>\n",
       "      <td>0.175860</td>\n",
       "      <td>0.131166</td>\n",
       "      <td>0.180692</td>\n",
       "      <td>0.167998</td>\n",
       "      <td>-0.017413</td>\n",
       "      <td>-0.082512</td>\n",
       "      <td>-0.050906</td>\n",
       "      <td>-0.178924</td>\n",
       "      <td>-0.039019</td>\n",
       "      <td>-0.024834</td>\n",
       "      <td>-0.076799</td>\n",
       "      <td>-0.027626</td>\n",
       "      <td>-0.221608</td>\n",
       "      <td>-0.056086</td>\n",
       "      <td>0.155066</td>\n",
       "      <td>0.148291</td>\n",
       "      <td>-0.087044</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.199185</td>\n",
       "      <td>0.168463</td>\n",
       "      <td>0.192281</td>\n",
       "      <td>0.141891</td>\n",
       "      <td>0.191477</td>\n",
       "      <td>0.183483</td>\n",
       "      <td>0.072232</td>\n",
       "      <td>0.006426</td>\n",
       "      <td>0.041366</td>\n",
       "      <td>-0.068816</td>\n",
       "      <td>0.047534</td>\n",
       "      <td>0.071780</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>-0.129734</td>\n",
       "      <td>0.017414</td>\n",
       "      <td>0.243052</td>\n",
       "      <td>0.269320</td>\n",
       "      <td>0.013745</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.349332</td>\n",
       "      <td>0.446937</td>\n",
       "      <td>0.403140</td>\n",
       "      <td>0.236072</td>\n",
       "      <td>0.316970</td>\n",
       "      <td>0.358551</td>\n",
       "      <td>0.866725</td>\n",
       "      <td>0.567826</td>\n",
       "      <td>0.928020</td>\n",
       "      <td>0.879607</td>\n",
       "      <td>0.597621</td>\n",
       "      <td>0.754592</td>\n",
       "      <td>0.924048</td>\n",
       "      <td>0.922969</td>\n",
       "      <td>0.559176</td>\n",
       "      <td>0.468107</td>\n",
       "      <td>0.757425</td>\n",
       "      <td>1.248630</td>\n",
       "      <td>0.902202</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gender  SeniorCitizen       Partner    Dependents        tenure  \\\n",
       "count  10000.000000   10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.185042       0.156384      0.178105      0.131436      0.181296   \n",
       "std        0.026490       0.024163      0.027582      0.018869      0.020795   \n",
       "min        0.084527       0.051687      0.048179      0.054626      0.079699   \n",
       "25%        0.169330       0.142467      0.162137      0.120002      0.169915   \n",
       "50%        0.183725       0.154332      0.175860      0.131166      0.180692   \n",
       "75%        0.199185       0.168463      0.192281      0.141891      0.191477   \n",
       "max        0.349332       0.446937      0.403140      0.236072      0.316970   \n",
       "\n",
       "       PhoneService  MultipleLines  InternetService  OnlineSecurity  \\\n",
       "count  10000.000000   10000.000000     10000.000000    10000.000000   \n",
       "mean       0.167736      -0.016865        -0.105815       -0.053186   \n",
       "std        0.028070       0.151935         0.184663        0.169728   \n",
       "min        0.059483      -0.834261        -1.168748       -0.854694   \n",
       "25%        0.150546      -0.102229        -0.197091       -0.149690   \n",
       "50%        0.167998      -0.017413        -0.082512       -0.050906   \n",
       "75%        0.183483       0.072232         0.006426        0.041366   \n",
       "max        0.358551       0.866725         0.567826        0.928020   \n",
       "\n",
       "       OnlineBackup  DeviceProtection   TechSupport   StreamingTV  \\\n",
       "count  10000.000000      10000.000000  10000.000000  10000.000000   \n",
       "mean      -0.172749         -0.059698     -0.030590     -0.077091   \n",
       "std        0.191974          0.173430      0.172230      0.171695   \n",
       "min       -1.199198         -1.771493     -0.880217     -1.088684   \n",
       "25%       -0.286428         -0.147699     -0.129361     -0.173878   \n",
       "50%       -0.178924         -0.039019     -0.024834     -0.076799   \n",
       "75%       -0.068816          0.047534      0.071780      0.018550   \n",
       "max        0.879607          0.597621      0.754592      0.924048   \n",
       "\n",
       "       StreamingMovies      Contract  PaperlessBilling  PaymentMethod  \\\n",
       "count     10000.000000  10000.000000      10000.000000   10000.000000   \n",
       "mean         -0.028659     -0.241728         -0.076758       0.151002   \n",
       "std           0.159302      0.182624          0.142371       0.155359   \n",
       "min          -0.824813     -1.328450         -0.978064      -0.673684   \n",
       "25%          -0.121562     -0.335840         -0.155375       0.062328   \n",
       "50%          -0.027626     -0.221608         -0.056086       0.155066   \n",
       "75%           0.062000     -0.129734          0.017414       0.243052   \n",
       "max           0.922969      0.559176          0.468107       0.757425   \n",
       "\n",
       "       MonthlyCharges  TotalCharges         Churn  \n",
       "count    10000.000000  10000.000000  10000.000000  \n",
       "mean         0.155057     -0.080559      0.504300  \n",
       "std          0.202496      0.165470      0.500006  \n",
       "min         -0.813255     -0.926193      0.000000  \n",
       "25%          0.039139     -0.182759      0.000000  \n",
       "50%          0.148291     -0.087044      1.000000  \n",
       "75%          0.269320      0.013745      1.000000  \n",
       "max          1.248630      0.902202      1.000000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate 10,000 random datapoints\n",
    "num_samples = 10000\n",
    "noise = torch.randn(num_samples, 10).to(device)\n",
    "data_class = torch.randint(0, 2, (num_samples, )).float().to(device)\n",
    "generated_data = generator(noise, data_class)\n",
    "\n",
    "# Convert the generated data to a pandas DataFrame\n",
    "generated_df = pd.DataFrame(generated_data.cpu().detach().numpy(), columns=X.columns)\n",
    "generated_df['Churn'] = data_class.cpu().detach().numpy()\n",
    "\n",
    "generated_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
